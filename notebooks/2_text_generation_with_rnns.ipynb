{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with RNNs\n",
    "\n",
    "In this notebook we use the components developed in `modelling.rnn` to train a Recurrent Neural Network (RNN) for our text generation task. RNNs are easier to train than models based on transformers and can such serve as a useful benchmark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The bulk of the code required to setup, train and generate new text from the model, is contained within `modelling.rnn` (check the source code for the details). We import this module together with others that serve the training data and manage model persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modelling import data as data\n",
    "from modelling import rnn as rnn\n",
    "from modelling import utils as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Parameters\n",
    "\n",
    "Configure hyper-parameters for the model and the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lstm_next_word_gen\"\n",
    "\n",
    "SIZE_EMBED = 256\n",
    "SIZE_HIDDEN = 512\n",
    "\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "SEQ_LEN = 40\n",
    "MIN_WORD_FREQ = 2\n",
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.FilmReviewSequences(split=\"all\", seq_len=SEQ_LEN, min_freq=MIN_WORD_FREQ)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=data.pad_seq2seq_data\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextWordPredictionRNN(\n",
       "  (_embedding): Embedding(63223, 256)\n",
       "  (_lstm): LSTM(256, 512, batch_first=True)\n",
       "  (_linear): Linear(in_features=512, out_features=63223, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnn.NextWordPredictionRNN(dataset.vocab_size, SIZE_EMBED, SIZE_HIDDEN)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple one layer LSTM that works on sequences of token embeddings that are learnt concurrently with the rest of the model. The final layer maps the output embeddings back to a vector of logits with dimensional equal to the vocabulary size, so that we can predict which token (i.e, category) the next word in the sequence belongs to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 current loss = 5.3521: 100%|██████████| 195/195 [04:56<00:00,  1.52s/it]\n",
      "epoch 2 current loss = 4.8724: 100%|██████████| 195/195 [04:59<00:00,  1.54s/it]\n",
      "epoch 3 current loss = 4.6255: 100%|██████████| 195/195 [04:59<00:00,  1.54s/it]\n",
      "epoch 4 current loss = 4.3961: 100%|██████████| 195/195 [04:59<00:00,  1.54s/it]\n",
      "epoch 5 current loss = 4.1761: 100%|██████████| 195/195 [04:58<00:00,  1.53s/it]\n",
      "epoch 6 current loss = 3.8963: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 7 current loss = 3.7594: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 8 current loss = 3.6293: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 9 current loss = 3.4493: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 10 current loss = 3.3112: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 11 current loss = 3.2383: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 12 current loss = 3.1164: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 13 current loss = 3.0496: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 14 current loss = 2.9714: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 15 current loss = 2.9007: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 16 current loss = 2.8591: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 17 current loss = 2.7622: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 18 current loss = 2.7228: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 19 current loss = 2.6749: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n",
      "epoch 20 current loss = 2.5923: 100%|██████████| 195/195 [04:55<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best model:\n",
      "|-- epoch: 20\n",
      "|-- loss: 2.5923\n"
     ]
    }
   ],
   "source": [
    "train_losses = rnn.train(model, data_loader, N_EPOCHS, LEARNING_RATE)\n",
    "utils.save_model(model, name=MODEL_NAME, loss=min(train_losses.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading a model and instantiating a tokeniser that can also map from tokens back to text. The `load_model` function will load the best performing model that has been persisted on the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading .models/lstm_next_word_gen/trained@2023-06-30T03:22:57;loss=2_5923.pt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = data.IMDBTokenizer()\n",
    "best_model: rnn.NextWordPredictionRNN = utils.load_model(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass a prompt to the model and get it to generate the text that comes after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> I THOUGHT THIS MOVIE WAS great. I mean it s not even a good movie but it is a great\n",
      "film. The acting is good but the execution is appallingly acted and the story is about a\n",
      "mercenary boy sean connery who seems to be the kind of relationship between the struggles\n",
      "of the characters. No real suspense. The plot is...\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I thought this movie was\"\n",
    "text = rnn.generate(best_model, prompt, tokenizer, temperature=2.0)\n",
    "\n",
    "for line in wrap(text, width=89):\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare this output with that from an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> I THOUGHT THIS MOVIE WAS scoutmaster magnetic meaney julliard chrissy liven raju\n",
      "mcmansions coneheads sh_it huac harding shelby draw liquids gratified cass mayer overs\n",
      "perú deflate approx escapee gouged rodriquez terrestial pressburger safes sloshed\n",
      "transient manji daimajin sumatra concludes mili leitch burtonesque nudged chromosomes\n",
      "tearjerker beaned insidious interloper cartland duchovney vie drown cheshire shrews\n",
      "peacecraft woodman brueghel stimuli adventuring continental described newsreels ballast\n",
      "kara hitlerian...\n"
     ]
    }
   ],
   "source": [
    "untrained_model = rnn.NextWordPredictionRNN(dataset.vocab_size, SIZE_EMBED, SIZE_HIDDEN)\n",
    "text = rnn.generate(untrained_model, prompt, tokenizer, temperature=2.0)\n",
    "\n",
    "for line in wrap(text, width=89):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
