{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with RNNs\n",
    "\n",
    "In this notebook we use the components developed in `modelling.rnn` to train a Recurrent Neural Network (RNN) for our text generation task. RNNs are easier to train than models based on transformers and can such serve as a useful benchmark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The bulk of the code required to setup, train and generate new text from the model, is contained within `modelling.rnn` (check the source code for the details). We import this module together with others that serve the training data and manage model persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modelling import data as data\n",
    "from modelling import rnn as rnn\n",
    "from modelling import utils as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Parameters\n",
    "\n",
    "Configure hyper-parameters for the model and the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lstm_next_word_gen\"\n",
    "\n",
    "SIZE_EMBED = 256\n",
    "SIZE_HIDDEN = 512\n",
    "\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "SEQ_LEN = 40\n",
    "MIN_WORD_FREQ = 2\n",
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.FilmReviewSequences(split=\"all\", seq_len=SEQ_LEN, min_freq=MIN_WORD_FREQ)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=data.pad_seq2seq_data\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextWordPredictionRNN(\n",
       "  (_embedding): Embedding(63223, 256)\n",
       "  (_lstm): LSTM(256, 512, batch_first=True)\n",
       "  (_linear): Linear(in_features=512, out_features=63223, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnn.NextWordPredictionRNN(dataset.vocab_size, SIZE_EMBED, SIZE_HIDDEN)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple one layer LSTM that works on sequences of token embeddings that are learnt concurrently with the rest of the model. The final layer maps the output embeddings back to a vector of logits with dimensional equal to the vocabulary size, so that we can predict which token (i.e, category) the next word in the sequence belongs to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [04:54<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30T00:31:45 epoch 1 loss: 5.3390\n",
      "- best model:\n",
      "|-- epoch: 1\n",
      "|-- loss: 5.3390\n"
     ]
    }
   ],
   "source": [
    "train_losses = rnn.train(model, data_loader, N_EPOCHS, LEARNING_RATE)\n",
    "utils.save_model(model, name=MODEL_NAME, loss=min(train_losses.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading a model and instantiating a tokeniser that can also map from tokens back to text. The `load_model` function will load the best performing model that has been persisted on the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading .models/lstm_next_word_gen/trained@2023-06-30T00:15:29;loss=2_4333.pt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = data.IMDBTokenizer()\n",
    "best_model: rnn.NextWordPredictionRNN = utils.load_model(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass a prompt to the model and get it to generate the text that comes after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> I THOUGHT THIS MOVIE WAS not going to be a comedy but was quite good it was a big\n",
      "release from the beginning and the ending was really cool. I was so disappointed. The\n",
      "story is about a young man who hopes to fly in the <unk> to find partners for her\n",
      "beautiful women in this obnoxious show. He s a fugitive...\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I thought this movie was\"\n",
    "text = rnn.generate(best_model, prompt, tokenizer, temperature=2.0)\n",
    "\n",
    "for line in wrap(text, width=89):\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare this output with that from an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> I THOUGHT THIS MOVIE WAS scoutmaster magnetic meaney julliard chrissy liven raju\n",
      "mcmansions coneheads sh_it huac harding shelby draw liquids gratified cass mayer overs\n",
      "perú deflate approx escapee gouged rodriquez terrestial pressburger safes sloshed\n",
      "transient manji daimajin sumatra concludes mili leitch burtonesque nudged chromosomes\n",
      "tearjerker beaned insidious interloper cartland duchovney vie drown cheshire shrews\n",
      "peacecraft woodman brueghel stimuli adventuring continental described newsreels ballast\n",
      "kara hitlerian...\n"
     ]
    }
   ],
   "source": [
    "untrained_model = rnn.NextWordPredictionRNN(dataset.vocab_size, SIZE_EMBED, SIZE_HIDDEN)\n",
    "text = rnn.generate(untrained_model, prompt, tokenizer, temperature=2.0)\n",
    "\n",
    "for line in wrap(text, width=89):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
