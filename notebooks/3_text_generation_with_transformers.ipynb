{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Transformers\n",
    "\n",
    "In this notebook we use the components developed in `modelling.transformer` to train a transformer decoder for our text generation task. We will compare the performance of this model with that established by our RNN baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The bulk of the code required to setup, train and generate new text from the model, is contained within `modelling.transformer` (check the source code for the details). We import this module together with others that serve the training data and manage model persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modelling import data as data\n",
    "from modelling import transformer as tfr\n",
    "from modelling import utils as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Parameters\n",
    "\n",
    "Configure hyper-parameters for the model and the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"decoder_next_word_gen\"\n",
    "\n",
    "SIZE_EMBED = 256\n",
    "\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 40\n",
    "MIN_WORD_FREQ = 2\n",
    "MAXIMUM_LEARNING_RATE = 0.001\n",
    "WARMUP_EPOCHS = 2\n",
    "GRADIENT_CLIP = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = data.make_sequence_datasets(seq_len=SEQ_LEN, min_word_freq=MIN_WORD_FREQ)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    datasets.train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=data.pad_seq2seq_data\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextWordPredictionTransformer(\n",
       "  (_position_encoder): PositionalEncoding(\n",
       "    (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_embedding): Embedding(63223, 256)\n",
       "  (_decoder): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_linear): Linear(in_features=256, out_features=63223, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tfr.NextWordPredictionTransformer(dataset.vocab_size, SIZE_EMBED)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare this model with the RNN model, then it is easy to see that this one is significantly more complex with many more layers (and thus parameters). We start with the same embedding layer albeit combined with a positional encoding, that is then fed into a transformer decoder layer comprised of two multi-head attention blocks, two linear (dense) feed-forward layers and three sets of layer normalisation and dropout."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "As well as having a far more complex architecture, transformer based models are also trickier to train successfully. In particular, the vast number of parameters can lead to gradients that can grow very large in the early stages of training, thus preventing convergence.\n",
    "\n",
    "We handle this using a learning rate schedule that starts close to zero and slowly ramps-up, before falling again as we reach the end of the desired number of epochs. We also clip the gradients - see the source code for the full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of warmup steps: 3122 / 31220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 current loss = 5.2357 (LR = 0.00049692): 100%|██████████| 1561/1561 [32:20<00:00,  1.24s/it]\n",
      "epoch 2 current loss = 5.0427 (LR = 0.00097553): 100%|██████████| 1561/1561 [32:45<00:00,  1.26s/it]\n",
      "epoch 3 current loss = 4.7089 (LR = 0.00094550): 100%|██████████| 1561/1561 [32:46<00:00,  1.26s/it]\n",
      "epoch 4 current loss = 4.5000 (LR = 0.00090451): 100%|██████████| 1561/1561 [32:46<00:00,  1.26s/it]\n",
      "epoch 5 current loss = 4.3209 (LR = 0.00085355): 100%|██████████| 1561/1561 [32:45<00:00,  1.26s/it]\n",
      "epoch 6 current loss = 4.2717 (LR = 0.00079389): 100%|██████████| 1561/1561 [32:47<00:00,  1.26s/it]\n",
      "epoch 7 current loss = 3.9115 (LR = 0.00072700): 100%|██████████| 1561/1561 [32:45<00:00,  1.26s/it]\n",
      "epoch 8 current loss = 3.7375 (LR = 0.00065451): 100%|██████████| 1561/1561 [32:46<00:00,  1.26s/it]\n",
      "epoch 9 current loss = 3.8571 (LR = 0.00057822): 100%|██████████| 1561/1561 [32:45<00:00,  1.26s/it]\n",
      "epoch 10 current loss = 3.4806 (LR = 0.00050000): 100%|██████████| 1561/1561 [32:46<00:00,  1.26s/it]\n",
      "epoch 11 current loss = 3.2400 (LR = 0.00042178): 100%|██████████| 1561/1561 [32:21<00:00,  1.24s/it]\n",
      "epoch 12 current loss = 3.1777 (LR = 0.00034549): 100%|██████████| 1561/1561 [32:41<00:00,  1.26s/it]\n",
      "epoch 13 current loss = 2.9169 (LR = 0.00027300): 100%|██████████| 1561/1561 [32:28<00:00,  1.25s/it]\n",
      "epoch 14 current loss = 3.0918 (LR = 0.00020611): 100%|██████████| 1561/1561 [32:47<00:00,  1.26s/it]\n",
      "epoch 15 current loss = 2.9770 (LR = 0.00014645): 100%|██████████| 1561/1561 [32:35<00:00,  1.25s/it]\n",
      "epoch 16 current loss = 3.1252 (LR = 0.00009549): 100%|██████████| 1561/1561 [32:35<00:00,  1.25s/it]\n",
      "epoch 17 current loss = 2.3962 (LR = 0.00005450): 100%|██████████| 1561/1561 [32:23<00:00,  1.25s/it]\n",
      "epoch 18 current loss = 2.7057 (LR = 0.00002447): 100%|██████████| 1561/1561 [32:46<00:00,  1.26s/it]\n",
      "epoch 19 current loss = 2.6981 (LR = 0.00000616): 100%|██████████| 1561/1561 [32:48<00:00,  1.26s/it]\n",
      "epoch 20 current loss = 2.7500 (LR = 0.00000000): 100%|██████████| 1561/1561 [32:47<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best model:\n",
      "|-- epoch: 17\n",
      "|-- loss: 2.3962\n"
     ]
    }
   ],
   "source": [
    "train_losses = tfr.train(\n",
    "        model,\n",
    "        data_loader,\n",
    "        N_EPOCHS,\n",
    "        MAXIMUM_LEARNING_RATE,\n",
    "        WARMUP_EPOCHS,\n",
    "        GRADIENT_CLIP\n",
    "    )\n",
    "utils.save_model(model, name=MODEL_NAME, loss=min(train_losses.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_train_losses(train_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading a model and instantiating a tokenizer that can also map from tokens back to text. The `load_model` function will load the best performing model that has been persisted on the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading .models/decoder_next_word_gen/trained@2023-07-01T22:59:23;loss=2_3962.pt\n"
     ]
    }
   ],
   "source": [
    "best_model: tfr.NextWordPredictionTransformer = utils.load_model(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass a prompt to the model and get it to generate the text that comes after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> I THOUGHT THIS MOVIE WAS a great movie. It was awesome. I was very young and very\n",
      "cool. I was very disappointed. I really love the movie. I think the movie is very good. I\n",
      "can t even though...\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I thought this movie was interesting, but\"\n",
    "text = tfr.generate(best_model, prompt, datasets.tokenizer, temperature=2.0)\n",
    "\n",
    "for line in wrap(text, width=89):\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare this output with that from an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> I THOUGHT THIS MOVIE WAS scoutmaster magnetic meaney julliard chrissy corine raju\n",
      "mcmansions coneheads sh_it replicant dwellers shelby draw liquids gratified cass mayer\n",
      "overs perú deflate approx escapee gouged rodriquez terrestial pressburger safes sloshed\n",
      "roy manji daimajin sumatra concludes mili leitch burtonesque nudged chromosomes\n",
      "tearjerker...\n"
     ]
    }
   ],
   "source": [
    "untrained_model = tfr.NextWordPredictionTransformer(dataset.tokenizer.vocab_size, SIZE_EMBED)\n",
    "text = tfr.generate(untrained_model, prompt, datasets.tokenizer, temperature=2.0)\n",
    "\n",
    "for line in wrap(text, width=89):\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
