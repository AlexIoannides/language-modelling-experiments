{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Attention Mechanisms to Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Sorry Dave, I'm afraid I can't do that.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14062, 19814,  1096,  2874,  9866, 17985,  3681,   685])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise(text: str) -> torch.Tensor:\n",
    "    words = text.split(\" \")\n",
    "    return torch.randint(0, 20000, [len(words)])\n",
    "\n",
    "\n",
    "tokenised_sentence = tokenise(sentence)\n",
    "tokenised_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert from tokens to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 32\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "embedded_tokens = embedding_layer(tokenised_sentence)\n",
    "embedded_tokens.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Self-Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the self-similarity matrix between all the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens = len(tokenised_sentence)\n",
    "\n",
    "attn_weights = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights[i, j] = torch.dot(embedded_tokens[i], embedded_tokens[j])\n",
    "\n",
    "attn_weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done with matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_matmul = torch.matmul(embedded_tokens, embedded_tokens.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is easy to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attn_weights_matmul, attn_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renormalise the rows, so that they sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_norm = F.softmax(attn_weights, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings = torch.matmul(attn_weights_norm, embedded_tokens)\n",
    "\n",
    "context_weighted_embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate manual computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5963, -0.2296,  0.3928, -1.0474, -1.5370, -0.4205, -0.5213,  0.6360,\n",
       "         0.2418, -0.9080, -0.4428, -0.5328, -0.6034, -1.2227,  0.2397,  0.3942,\n",
       "        -0.7501,  0.3245, -0.0228,  0.1374, -0.2983, -0.2089,  0.6657, -0.1347,\n",
       "        -0.3584,  1.4458,  1.4827, -0.3337, -1.1085,  0.4091,  1.9441, -0.3650],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_3 = (\n",
    "    attn_weights_norm[3, 0] * embedded_tokens[0] +\n",
    "    attn_weights_norm[3, 1] * embedded_tokens[1] +\n",
    "    attn_weights_norm[3, 2] * embedded_tokens[2] +\n",
    "    attn_weights_norm[3, 3] * embedded_tokens[3] +\n",
    "    attn_weights_norm[3, 4] * embedded_tokens[4] +\n",
    "    attn_weights_norm[3, 5] * embedded_tokens[5] +\n",
    "    attn_weights_norm[3, 6] * embedded_tokens[6] +\n",
    "    attn_weights_norm[3, 7] * embedded_tokens[7]\n",
    ")\n",
    "\n",
    "context_weighted_embeddings_3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(context_weighted_embeddings_3, context_weighted_embeddings[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Masking\n",
    "\n",
    "Build a causal masking matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "causal_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mask to the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.2212e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-1.2375e+01,  3.6565e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.0231e+00, -6.2132e+00,  2.8644e+01,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 3.4108e+00, -2.1306e+00,  1.2248e+00,  1.9428e+01,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-2.4254e+00,  1.3699e+00, -1.3215e+00, -4.4710e+00,  2.3533e+01,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-9.7495e+00,  1.0044e+01, -9.8557e+00,  5.0011e+00, -3.6843e+00,\n",
       "          3.6312e+01,  0.0000e+00,  0.0000e+00],\n",
       "        [-7.0159e+00, -6.5227e+00, -4.4904e+00, -1.0484e-02,  3.6967e+00,\n",
       "         -6.4772e-01,  3.0431e+01,  0.0000e+00],\n",
       "        [-5.0422e-01,  2.7397e+00, -4.4311e+00, -9.8397e-01, -7.0092e+00,\n",
       "          1.6973e-01, -7.5941e+00,  2.7720e+01]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights = attn_weights.masked_fill(causal_mask, 0.)\n",
    "\n",
    "causal_attn_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights_norm = F.softmax(causal_attn_weights, dim=1)\n",
    "causal_attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute causal context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings = torch.matmul(causal_attn_weights_norm, embedded_tokens)\n",
    "\n",
    "causal_context_weighted_embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate causal structure by computing manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings_3 = (\n",
    "    causal_attn_weights_norm[3, 0] * embedded_tokens[0] +\n",
    "    causal_attn_weights_norm[3, 1] * embedded_tokens[1] +\n",
    "    causal_attn_weights_norm[3, 2] * embedded_tokens[2] +\n",
    "    causal_attn_weights_norm[3, 3] * embedded_tokens[3]\n",
    ")\n",
    "\n",
    "torch.allclose(causal_context_weighted_embeddings_3, causal_context_weighted_embeddings[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrised Self-Attention\n",
    "\n",
    "Making attention ameanable to learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries, Keys and Values\n",
    "\n",
    "Let..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_q = torch.rand(n_tokens, n_tokens, requires_grad=True)\n",
    "u_k = torch.rand(n_tokens, n_tokens, requires_grad=True)\n",
    "u_v = torch.rand(n_tokens, n_tokens, requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such that,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.matmul(u_q, embedded_tokens)\n",
    "k = torch.matmul(u_k, embedded_tokens)\n",
    "v = torch.matmul(u_v, embedded_tokens)\n",
    "\n",
    "q.shape == embedded_tokens.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute context aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_param = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights_param[i, j] = torch.dot(q[i], k[j])\n",
    "\n",
    "attn_weights_param_norm = F.softmax(attn_weights_param, dim=1)\n",
    "context_weighted_embeddings_param = torch.matmul(attn_weights_param_norm, v)\n",
    "\n",
    "context_weighted_embeddings_param.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_param_3 = (\n",
    "    attn_weights_param_norm[3, 0] * v[0] +\n",
    "    attn_weights_param_norm[3, 1] * v[1] +\n",
    "    attn_weights_param_norm[3, 2] * v[2] +\n",
    "    attn_weights_param_norm[3, 3] * v[3] +\n",
    "    attn_weights_param_norm[3, 4] * v[4] +\n",
    "    attn_weights_param_norm[3, 5] * v[5] +\n",
    "    attn_weights_param_norm[3, 6] * v[6] +\n",
    "    attn_weights_param_norm[3, 7] * v[7]\n",
    ")\n",
    "\n",
    "torch.allclose(context_weighted_embeddings_param_3, context_weighted_embeddings_param[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(\n",
    "        query: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        values: torch.Tensor,\n",
    "        causal_masking: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Compute single attention head.\"\"\"\n",
    "    n_tokens, embedding_dim = query.shape\n",
    "    attn_weights_norm = torch.matmul(query, keys.T).softmax(dim=1)\n",
    "    if causal_masking:\n",
    "        mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "        attn_weights_norm = attn_weights_norm.masked_fill(mask, 0.)\n",
    "    context_weighted_embeddings = torch.matmul(attn_weights_norm, values)\n",
    "    return context_weighted_embeddings\n",
    "\n",
    "\n",
    "attn_head_out = attention(q, k, v)\n",
    "attn_head_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple heads..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_head_attention(\n",
    "    x_q: torch.Tensor,\n",
    "    x_k: torch.Tensor,\n",
    "    x_v: torch.Tensor,\n",
    "    n_heads: int,\n",
    "    causal_masking: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computing multiple attention heads.\"\"\"\n",
    "    n_tokens, embedding_dim = embedded_tokens.shape\n",
    "\n",
    "    u_q = torch.rand(n_heads, n_tokens, n_tokens, requires_grad=True)\n",
    "    u_k = torch.rand(n_heads, n_tokens, n_tokens, requires_grad=True)\n",
    "    u_v = torch.rand(n_heads, n_tokens, n_tokens, requires_grad=True)\n",
    "    w_out = torch.rand(n_heads*embedding_dim, embedding_dim, requires_grad=True)\n",
    "\n",
    "    attn_head_outputs = torch.concat(\n",
    "        [attention(u_q[h] @ x_q, u_k[h] @ x_k, u_v[h] @ x_v) for h in range(n_heads)],\n",
    "         dim=1\n",
    "    )\n",
    "\n",
    "    return torch.matmul(attn_head_outputs, w_out)\n",
    "\n",
    "\n",
    "multi_head_attn_out = multi_head_attention(embedded_tokens, embedded_tokens, embedded_tokens, n_heads=3)\n",
    "multi_head_attn_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Assembling a transforner decoder network to demonstrate the use of multi-head attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_decoder_layer(\n",
    "        src_embedding: torch.Tensor,\n",
    "        target_embedding: torch.Tensor,\n",
    "        n_heads: int,\n",
    "        causal_masking: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Assemble a transformer decoder layer from \"\"\"\n",
    "    x1 = multi_head_attention(target_embedding, target_embedding, target_embedding, n_heads)\n",
    "    x1 = F.layer_norm(x1 + target_embedding, x1.shape)\n",
    "    x2 = multi_head_attention(src_embedding, src_embedding, x1, n_heads)\n",
    "    x2 = F.layer_norm(x2 + x1, x1.shape)\n",
    "\n",
    "    linear_1 = nn.Linear(embedding_dim, 2*embedding_dim)\n",
    "    linear_2 = nn.Linear(2*embedding_dim, embedding_dim)\n",
    "\n",
    "    return F.relu(linear_2(linear_1(x2)))\n",
    "\n",
    "\n",
    "transformer_output = transformer_decoder_layer(embedded_tokens, embedded_tokens, n_heads=2)\n",
    "transformer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
