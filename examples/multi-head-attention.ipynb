{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms and Transformers\n",
    "\n",
    "This notebook demonstrates how attention mechanisms can be implemente and how they are used within transformer architectures. We will develop an understanding from first principles using PyTorch (no prior knowledge is requried).\n",
    "\n",
    "Attention mechanisms in deep learning aim to map a basic sequence of [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) into another sequence of embeddings that represent each word conditioned on the 'derived context' of that word within the text.\n",
    "\n",
    "We could express this mapping mathematically as, $\\textbf{x} \\to \\textbf{z} = f(\\textbf{x})$, where $\\textbf{x} = (\\vec{x_{1}}, ..., \\vec{x_{N}})$, $\\textbf{z} = (\\vec{z_{1}}, ..., \\vec{z_{N}})$, $\\vec{x}$ and $\\vec{z}$ are individual embedding vectors, and $N$ is the number of tokens in the sequence. The goal of attention is to learn $f$ from data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing PyTorch\n",
    "\n",
    "We will mostly be using PyTorch like NumPy (to create and manipulate tensors), but we will also use one or two modules from its neural networks module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Sorry Dave, I'm afraid I can't do that.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then [tokenise](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) our sentence into a sequence of integer values (one for each word), using an imaginary tokenisation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17733, 13358,  3783,  4256,  6665, 10168,  2821, 16004])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise(text: str, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"Dummy text tokeniser.\"\"\"\n",
    "    words = text.split(\" \")\n",
    "    return torch.randint(0, vocab_size, [len(words)])\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenised_sentence = tokenise(sentence, VOCAB_SIZE)\n",
    "n_tokens = len(tokenised_sentence)\n",
    "\n",
    "tokenised_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And embed each token into a vector space (as a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "\n",
    "embedding_layer = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "embedded_tokens = embedding_layer(tokenised_sentence)\n",
    "embedded_tokens.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these embeddings will need to be learnt when training any model that uses an embedding layer. We can compute the number of parameters in this layers as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of embedding parameters = 640,000\n"
     ]
    }
   ],
   "source": [
    "n_embedding_params = sum(len(p.flatten()) for p in embedding_layer.parameters())\n",
    "print(f\"number of embedding parameters = {n_embedding_params:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Self-Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to computing attention is to express the new 'contextual embeddings' as a weighted linear combination or the input imbeddings - e.g., $\\vec{x_{i}} \\to \\vec{z_{i}} = \\sum_{j=1}^{N}{a_{ij} \\times \\vec{x_{j}}}$. We then can focus on strategies for computing the weights.\n",
    "\n",
    "A sensible approach to computing the weights is to use the vector [dot product](https://en.wikipedia.org/wiki/Dot_product) between the embedding vectors - e.g., $a_{ij} = x_{i}^{T} \\cdot x_{i}$. This will lead to weights that are higher for embedding vectors that are closer to one another in the embedding space (i.e., are semantically closer), and vice versa. We can compute these weights as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights[i, j] = torch.dot(embedded_tokens[i], embedded_tokens[j])\n",
    "\n",
    "attn_weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention weights can also be computed more efficiently using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_matmul = torch.matmul(embedded_tokens, embedded_tokens.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can verify that the two approaches are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attn_weights_matmul, attn_weights, atol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to implementing this in practice the weights are scaled by the embedding dimension, and subsequently renormalised to sum to one across rows using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function). Steps like these make models easier to train by normalising the magnitude of gradients used within [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). For more insight into this refer to [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_norm = F.softmax(attn_weights / math.sqrt(EMBEDDING_DIM), dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the rows sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the final context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings = torch.matmul(attn_weights_norm, embedded_tokens)\n",
    "context_weighted_embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the embeddings are working as we expect by computing one manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5963, -0.2296,  0.3928, -1.0474, -1.5370, -0.4205, -0.5213,  0.6360,\n",
       "         0.2418, -0.9080, -0.4428, -0.5328, -0.6034, -1.2227,  0.2397,  0.3942,\n",
       "        -0.7501,  0.3245, -0.0228,  0.1374, -0.2983, -0.2089,  0.6657, -0.1347,\n",
       "        -0.3584,  1.4458,  1.4827, -0.3337, -1.1085,  0.4091,  1.9441, -0.3650],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_3 = (\n",
    "    attn_weights_norm[3, 0] * embedded_tokens[0] +\n",
    "    attn_weights_norm[3, 1] * embedded_tokens[1] +\n",
    "    attn_weights_norm[3, 2] * embedded_tokens[2] +\n",
    "    attn_weights_norm[3, 3] * embedded_tokens[3] +\n",
    "    attn_weights_norm[3, 4] * embedded_tokens[4] +\n",
    "    attn_weights_norm[3, 5] * embedded_tokens[5] +\n",
    "    attn_weights_norm[3, 6] * embedded_tokens[6] +\n",
    "    attn_weights_norm[3, 7] * embedded_tokens[7]\n",
    ")\n",
    "\n",
    "context_weighted_embeddings_3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verifying the output againt the matrix multiplication cmoputed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(context_weighted_embeddings_3, context_weighted_embeddings[3], atol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Masking\n",
    "\n",
    "You may haven noticed that the embedding vector for the first word, $\\vec{x_{1}}$, is mapped to a vector $\\vec{z_{1}}$, that is a function of embedding vectors for words that come after the first word. This isn't a problem if all we're intrested in doing is creating embeddings (or sequences) based on whole passages of text. It does pose a problem, however, if we're trying to develop a model that can generate new sequences given an initital sequence (or prompt). This problem is solved by using causal masking.\n",
    "\n",
    "Causal masking matrices can be constructed to flag which attention weights should be set to zero as they contain information that would break the causal relationships between embeddings. For example, in our setup we woudl have,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "causal_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we would apply to the attention weights as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.5003,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights = attn_weights.masked_fill(causal_mask, 0.)\n",
    "causal_attn_weights[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scaling and normalisation applied as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights_norm = F.softmax(causal_attn_weights / math.sqrt(EMBEDDING_DIM), dim=1)\n",
    "causal_attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we can compute causal context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings = torch.matmul(causal_attn_weights_norm, embedded_tokens)\n",
    "causal_context_weighted_embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can demonstrate the causal structure explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings_3 = (\n",
    "    causal_attn_weights_norm[3, 0] * embedded_tokens[0] +\n",
    "    causal_attn_weights_norm[3, 1] * embedded_tokens[1] +\n",
    "    causal_attn_weights_norm[3, 2] * embedded_tokens[2] +\n",
    "    causal_attn_weights_norm[3, 3] * embedded_tokens[3]\n",
    ")\n",
    "\n",
    "torch.allclose(causal_context_weighted_embeddings_3, causal_context_weighted_embeddings[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrised Self-Attention\n",
    "\n",
    "Making attention ameanable to learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries, Keys and Values\n",
    "\n",
    "Let..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_q = torch.rand(n_tokens, n_tokens, requires_grad=True)\n",
    "u_k = torch.rand(n_tokens, n_tokens, requires_grad=True)\n",
    "u_v = torch.rand(n_tokens, n_tokens, requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such that,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.matmul(u_q, embedded_tokens)\n",
    "k = torch.matmul(u_k, embedded_tokens)\n",
    "v = torch.matmul(u_v, embedded_tokens)\n",
    "\n",
    "q.shape == embedded_tokens.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute context aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_param = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights_param[i, j] = torch.dot(q[i], k[j])\n",
    "\n",
    "attn_weights_param_norm = F.softmax(attn_weights_param, dim=1)\n",
    "context_weighted_embeddings_param = torch.matmul(attn_weights_param_norm, v)\n",
    "\n",
    "context_weighted_embeddings_param.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_param_3 = (\n",
    "    attn_weights_param_norm[3, 0] * v[0] +\n",
    "    attn_weights_param_norm[3, 1] * v[1] +\n",
    "    attn_weights_param_norm[3, 2] * v[2] +\n",
    "    attn_weights_param_norm[3, 3] * v[3] +\n",
    "    attn_weights_param_norm[3, 4] * v[4] +\n",
    "    attn_weights_param_norm[3, 5] * v[5] +\n",
    "    attn_weights_param_norm[3, 6] * v[6] +\n",
    "    attn_weights_param_norm[3, 7] * v[7]\n",
    ")\n",
    "\n",
    "torch.allclose(context_weighted_embeddings_param_3, context_weighted_embeddings_param[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(\n",
    "        query: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        values: torch.Tensor,\n",
    "        causal_masking: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Compute single attention head.\"\"\"\n",
    "    n_tokens, embedding_dim = query.shape\n",
    "    attn_weights_norm = torch.matmul(query, keys.T).softmax(dim=1)\n",
    "    if causal_masking:\n",
    "        mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "        attn_weights_norm = attn_weights_norm.masked_fill(mask, 0.)\n",
    "    context_weighted_embeddings = torch.matmul(attn_weights_norm, values)\n",
    "    return context_weighted_embeddings\n",
    "\n",
    "\n",
    "attn_head_out = attention(q, k, v)\n",
    "attn_head_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple heads..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_head_attention(\n",
    "    x_q: torch.Tensor,\n",
    "    x_k: torch.Tensor,\n",
    "    x_v: torch.Tensor,\n",
    "    n_heads: int,\n",
    "    causal_masking: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computing multiple attention heads.\"\"\"\n",
    "    n_tokens, embedding_dim = embedded_tokens.shape\n",
    "\n",
    "    u_q = torch.rand(n_heads, n_tokens, n_tokens, requires_grad=True)\n",
    "    u_k = torch.rand(n_heads, n_tokens, n_tokens, requires_grad=True)\n",
    "    u_v = torch.rand(n_heads, n_tokens, n_tokens, requires_grad=True)\n",
    "    w_out = torch.rand(n_heads*embedding_dim, embedding_dim, requires_grad=True)\n",
    "\n",
    "    attn_head_outputs = torch.concat(\n",
    "        [attention(u_q[h] @ x_q, u_k[h] @ x_k, u_v[h] @ x_v) for h in range(n_heads)],\n",
    "         dim=1\n",
    "    )\n",
    "\n",
    "    return torch.matmul(attn_head_outputs, w_out)\n",
    "\n",
    "\n",
    "multi_head_attn_out = multi_head_attention(embedded_tokens, embedded_tokens, embedded_tokens, n_heads=3)\n",
    "multi_head_attn_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Assembling a transforner decoder network to demonstrate the use of multi-head attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_decoder_layer(\n",
    "        src_embedding: torch.Tensor,\n",
    "        target_embedding: torch.Tensor,\n",
    "        n_heads: int,\n",
    "        causal_masking: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Assemble a transformer decoder layer from \"\"\"\n",
    "    x1 = multi_head_attention(target_embedding, target_embedding, target_embedding, n_heads)\n",
    "    x1 = F.layer_norm(x1 + target_embedding, x1.shape)\n",
    "    x2 = multi_head_attention(src_embedding, src_embedding, x1, n_heads)\n",
    "    x2 = F.layer_norm(x2 + x1, x1.shape)\n",
    "\n",
    "    linear_1 = nn.Linear(embedding_dim, 2*embedding_dim)\n",
    "    linear_2 = nn.Linear(2*embedding_dim, embedding_dim)\n",
    "\n",
    "    return F.relu(linear_2(linear_1(x2)))\n",
    "\n",
    "\n",
    "transformer_output = transformer_decoder_layer(embedded_tokens, embedded_tokens, n_heads=2)\n",
    "transformer_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources\n",
    "\n",
    "1. [PyTorch docs](https://pytorch.org/docs/stable/index.html)\n",
    "2. [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
